{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources : https://github.com/Bhavin789/Twitter-Airline-Sentiment-Analysis/blob/master/Air%20Line%20Twitter%20Sentiment%20Analysis.ipynb \n",
    "\n",
    "https://scikit-learn.org/dev/auto_examples/model_selection/grid_search_text_feature_extraction.html https://www.kaggle.com/jiashenliu/how-can-we-predict-the-sentiment-by-tweets\n",
    "\n",
    "https://medium.com/analytics-vidhya/twitter-sentiment-analysis-b9a12dbb2043\n",
    "\n",
    "https://colab.research.google.com/drive/1e1lRkOAqwmlPDsUaJ5uxzE-wU_Dty0fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@USAirways  ! THE WORST in customer service. @...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@united call wait times are over 20 minutes an...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@JetBlue what's up with the random delay on fl...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@AmericanAir Good morning!  Wondering why my p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@united UA 746. Pacific Rim and Date Night cut...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               text  Target\n",
       "0   1  @USAirways  ! THE WORST in customer service. @...      -1\n",
       "1   2  @united call wait times are over 20 minutes an...      -1\n",
       "2   3  @JetBlue what's up with the random delay on fl...      -1\n",
       "3   4  @AmericanAir Good morning!  Wondering why my p...       0\n",
       "4   5  @united UA 746. Pacific Rim and Date Night cut...      -1"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "# your code here\n",
    "data = pd.read_csv(r'E:\\Semester Three UTD\\NLP\\Homework1\\train.csv',encoding='latin1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['Target'].plot(kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifying the list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "print(len(stopwords.words('english')))\n",
    "remove_from_stopwords = [\"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"needn't\", \"shan't\" \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\",'not',\"who\", \"what\", \"when\", \"why\", \"how\", \"which\", \"where\", \"whom\", \"no\", \"not\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative words should be retained as they explain negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of stopwords in English:\n",
      "{'once', 'few', 'shan', 'then', 'but', 'were', 'into', 'against', 'all', 'these', 'between', 'now', 'did', \"she's\", 'am', 'be', 'she', 'have', 'theirs', 'through', 'at', 'hers', 'they', 'as', 'doing', 'has', 'won', 'very', 'my', \"you'll\", 'by', 'over', 'its', 'an', 'your', \"that'll\", 'me', 'there', 'any', 'their', 'do', 'them', 'that', 've', \"you're\", 'i', 'you', 'him', 'own', 'll', 'will', 'such', 'again', 'm', 'are', 'some', 'if', 'up', 'while', 'he', 'the', 'most', 'themselves', 'only', 'd', 'we', 'her', 'and', 'itself', 'both', 'should', 'of', 'myself', 'a', 'or', 'those', 'aren', 'further', 'yours', 'because', 'below', 'to', 'just', \"you'd\", 'than', 'his', 'is', 'yourself', 'ma', 'about', 'other', 'on', 'y', 'himself', 't', 'above', \"you've\", 'needn', 'ours', 'been', 'ourselves', 'down', 'having', 'each', 'can', 'was', 'our', 're', 'from', 'so', 'with', 'o', \"it's\", 'more', 'in', 'it', 'same', 'out', 'under', 'after', 'until', 'for', 'yourselves', 'herself', 's', 'this', 'had', \"should've\", 'during', 'being', 'before', 'nor', 'does', 'off', 'here', 'too'}\n",
      "136\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) - set([\"who\", \"what\", \"when\", \"why\", \"how\", \"which\", \"where\", \"whom\", \"no\", \"not\", \"weren't\", \"aren't\",\"didn't\", \"wasn't\", \"couldn't\", \"hadn't\",\"hasn't\", \"doesn't\", \"shouldn't\", \"isn't\", \"wouldn't\", \"don't\", \"mightn't\", \"won't\", \"haven't\", \"mustn\", \"ain\",\"hasn\", \"weren\", \"mustn't\", \"wasn\", \"didn\", \"hadn\", \"don\", \"haven\", \"shouldn\", \"shan't\", \"isn\", \"wouldn\", \"mightn\", \"couldn\", \"needn't\", \"doesn\"])\n",
    "print(\"\\nList of stopwords in English:\")\n",
    "print (stop_words)\n",
    "print (len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stop_words]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "data['text']=data['text'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextCounts(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def count_regex(self, pattern, tweet):\n",
    "        #finding all the substring containing the pattern in the tweet\n",
    "        return len(re.findall(pattern, tweet))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # fit method is used when specific operations need to be done on the train data, but not on the test data\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        #all the alphanumeric character\n",
    "        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n",
    "        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n",
    "        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n",
    "        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n",
    "        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?+', x))\n",
    "        count_urls = X.apply(lambda x: self.count_regex(r'https?://[^\\s]+[\\s]?', x))\n",
    "        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n",
    "        # Moreover, it will result in having more words in the tweet\n",
    "        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n",
    "        \n",
    "        data = pd.DataFrame({'count_words': count_words\n",
    "                           , 'count_mentions': count_mentions\n",
    "                           , 'count_hashtags': count_hashtags\n",
    "                           , 'count_capital_words': count_capital_words\n",
    "                           , 'count_excl_quest_marks': count_excl_quest_marks\n",
    "                           , 'count_urls': count_urls\n",
    "                           , 'count_emojis': count_emojis\n",
    "                          })\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0           11               0               0                    0   \n",
       "1            9               0               0                    0   \n",
       "2            8               0               0                    0   \n",
       "3           12               0               0                    0   \n",
       "4           12               0               0                    0   \n",
       "\n",
       "   count_excl_quest_marks  count_urls  count_emojis  \n",
       "0                       0           0             0  \n",
       "1                       0           0             0  \n",
       "2                       0           0             0  \n",
       "3                       0           0             0  \n",
       "4                       0           0             0  "
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tc = TextCounts()\n",
    "\n",
    "data_eda = tc.fit_transform(data.text)\n",
    "data_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0           11               0               0                    0   \n",
       "1            9               0               0                    0   \n",
       "2            8               0               0                    0   \n",
       "3           12               0               0                    0   \n",
       "4           12               0               0                    0   \n",
       "\n",
       "   count_excl_quest_marks  count_urls  count_emojis  Target  \n",
       "0                       0           0             0      -1  \n",
       "1                       0           0             0      -1  \n",
       "2                       0           0             0      -1  \n",
       "3                       0           0             0       0  \n",
       "4                       0           0             0      -1  "
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_eda['Target'] = data.Target\n",
    "data_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>3651.866404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3685.492188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3661.348112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id\n",
       "Target             \n",
       "-1      3651.866404\n",
       " 0      3685.492188\n",
       " 1      3661.348112"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Target').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>10.949190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.985026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.188013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "Target                                                                     \n",
       "-1        10.949190             0.0             0.0                  0.0   \n",
       " 0         7.985026             0.0             0.0                  0.0   \n",
       " 1         8.188013             0.0             0.0                  0.0   \n",
       "\n",
       "        count_excl_quest_marks  count_urls  count_emojis  \n",
       "Target                                                    \n",
       "-1                         0.0         0.0           0.0  \n",
       " 0                         0.0         0.0           0.0  \n",
       " 1                         0.0         0.0           0.0  "
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_eda.groupby('Target').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning the text- tried both Porter and Snowball stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')\n",
    "        return input_text.translate(trantab)\n",
    "\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "\n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        #stemmer2 = SnowballStemmer(language='english')\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.stemming)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records have no words left after text cleaning\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ct = CleanText()\n",
    "sr_clean = ct.fit_transform(data.text)\n",
    "sr_clean.sample(5)\n",
    "empty_clean = sr_clean == ''\n",
    "print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\n",
    "sr_clean.loc[empty_clean] = '[no_text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records have no words left after text cleaning\n",
      "0    usairway worst custom servic usairway call mon...\n",
      "1    unit call wait time minut airport wait time lo...\n",
      "2     jetblu what random delay flight chanc fals alarm\n",
      "3    americanair good morn wonder whi pre tsa check...\n",
      "4    unit pacif rim date night cut not constantli r...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "empty_clean = sr_clean == ''\n",
    "\n",
    "\n",
    "print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\n",
    "sr_clean.loc[empty_clean] = '[no_text]'\n",
    "print (sr_clean[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aadvantag',\n",
       " 'aafail',\n",
       " 'aaso',\n",
       " 'abandon',\n",
       " 'abassinet',\n",
       " 'abc',\n",
       " 'abcnetwork',\n",
       " 'abcnew',\n",
       " 'abi',\n",
       " 'abigailedg',\n",
       " 'abil',\n",
       " 'abl',\n",
       " 'aboard',\n",
       " 'aboout',\n",
       " 'abound',\n",
       " 'abq',\n",
       " 'absolut',\n",
       " 'absoulut',\n",
       " 'absurd',\n",
       " 'absurdli',\n",
       " 'abt',\n",
       " 'abus',\n",
       " 'abysm',\n",
       " 'acc',\n",
       " 'acceler',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accid',\n",
       " 'accident',\n",
       " 'accomid',\n",
       " 'accommod',\n",
       " 'accompani',\n",
       " 'accomplish',\n",
       " 'accord',\n",
       " 'accordingli',\n",
       " 'account',\n",
       " 'accountâ',\n",
       " 'acct',\n",
       " 'accur',\n",
       " 'achiev',\n",
       " 'aci',\n",
       " 'ack',\n",
       " 'acknowledg',\n",
       " 'acosta',\n",
       " 'acpt',\n",
       " 'acquir',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actingoutmgmnt',\n",
       " 'action']"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(sr_clean)\n",
    "#printing only first 50\n",
    "cv.get_feature_names()[0:50]\n",
    "#bow.sum(axis=0)\n",
    "\n",
    "#from sklearn.feature_extraction.text import HashingVectorizer\n",
    "#hv = HashingVectorizer()\n",
    "#bow = hv.fit_transform(sr_clean)\n",
    "#hv.get_feature_names()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_words',\n",
       " 'count_mentions',\n",
       " 'count_hashtags',\n",
       " 'count_capital_words',\n",
       " 'count_excl_quest_marks',\n",
       " 'count_urls',\n",
       " 'count_emojis',\n",
       " 'Target',\n",
       " 'clean_text']"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model = data_eda\n",
    "data_model['clean_text'] = sr_clean\n",
    "data_model.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        #print(\"I am in\")\n",
    "        \n",
    "        #print(X[self.cols].dtype())\n",
    "        return X[self.cols]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_model.drop('Target', axis=1), data_model.Target, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining gridsearch and vectorization used MacroF as scoring criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_vect(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None, is_w2v=False):\n",
    "    \n",
    "    textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'\n",
    "                      ,'count_mentions','count_urls','count_words']\n",
    "    \n",
    "    if is_w2v:\n",
    "        w2vcols = []\n",
    "        print(\"not good\")\n",
    "        for i in range(SIZE):\n",
    "            w2vcols.append(i)\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('w2v', ColumnExtractor(cols=w2vcols))]\n",
    "                                , n_jobs=-1)\n",
    "    else:\n",
    "        print(\"ok\")\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text')), ('vect', vect)]))]\n",
    "                                , n_jobs=1)\n",
    "\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', features)\n",
    "        , ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)\n",
    "\n",
    "    grid_s = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=6,scoring='f1_macro')\n",
    "    grid_s.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    print(\"Best CV score: %0.3f\" % grid_s.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_s.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_s.best_estimator_.score(X_test, y_test))\n",
    "    print(\"\\n\")\n",
    "    print(\"Train score with best_estimator_: %0.3f\" % grid_s.best_estimator_.score(X_train, y_train))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report Test Data\")\n",
    "    print(classification_report(y_test, grid_s.best_estimator_.predict(X_test)))\n",
    "    print(\"Classification Report Train Data\")\n",
    "    print(classification_report(y_train, grid_s.best_estimator_.predict(X_train)))                    \n",
    "    return grid_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the vectorizers\n",
    "parameters_vect = {\n",
    "    'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
    "    'features__pipe__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'features__pipe__vect__min_df': (1,2,3,4)\n",
    "}\n",
    "\n",
    "\n",
    "# Parameters for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.25, 0.5, 0.75,1,1.25)\n",
    "}\n",
    "\n",
    "\n",
    "# Parameters for LogisticRegression\n",
    "parameters_logreg = {\n",
    "    \n",
    "    'clf__C': (1.05, 1.10, 1.15,1.20,1.25),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__multi_class': ['auto'],\n",
    "    #'clf_solver':['liblinear']\n",
    "}\n",
    "\n",
    "# Parameter grid settings for XGBoost \n",
    "parameters_xgboost = {\n",
    "    #'clf__min_child_weight': (1,5,10),\n",
    "    'clf__gamma': (0.5,1,1.5,2),\n",
    "}\n",
    "\n",
    "# Parameter grid settings for BernoulliNB\n",
    "parameters_bnb = {\n",
    "    'clf__alpha': [1],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using four Algorithms - Multinomial NB,Bernoulli NB,Logistic and XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "mnb = MultinomialNB()\n",
    "logreg = LogisticRegression()\n",
    "xgboost = XGBClassifier()\n",
    "Bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Executing models with Countvectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvect = CountVectorizer()\n",
    "#Hashvect = HashingVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes with CountVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Fitting 6 folds for each of 120 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   27.3s\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:   44.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.682\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.5\n",
      "\tfeatures__pipe__vect__max_df: 0.5\n",
      "\tfeatures__pipe__vect__min_df: 4\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 2)\n",
      "Test score with best_estimator_: 0.760\n",
      "\n",
      "\n",
      "Train score with best_estimator_: 0.846\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.86      0.85      1414\n",
      "           0       0.55      0.48      0.51       437\n",
      "           1       0.65      0.70      0.67       345\n",
      "\n",
      "    accuracy                           0.76      2196\n",
      "   macro avg       0.68      0.68      0.68      2196\n",
      "weighted avg       0.75      0.76      0.76      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.90      0.89      0.90      3152\n",
      "           0       0.74      0.70      0.72      1099\n",
      "           1       0.80      0.86      0.83       873\n",
      "\n",
      "    accuracy                           0.85      5124\n",
      "   macro avg       0.81      0.82      0.81      5124\n",
      "weighted avg       0.85      0.85      0.85      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb_cv = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=countvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bernoulli Naive Bayes with CountVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Fitting 6 folds for each of 24 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:   11.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.711\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 4\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.783\n",
      "\n",
      "\n",
      "Train score with best_estimator_: 0.841\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.87      0.87      0.87      1414\n",
      "           0       0.57      0.57      0.57       437\n",
      "           1       0.68      0.69      0.68       345\n",
      "\n",
      "    accuracy                           0.78      2196\n",
      "   macro avg       0.71      0.71      0.71      2196\n",
      "weighted avg       0.78      0.78      0.78      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.90      0.89      0.89      3152\n",
      "           0       0.71      0.72      0.71      1099\n",
      "           1       0.80      0.82      0.81       873\n",
      "\n",
      "    accuracy                           0.84      5124\n",
      "   macro avg       0.80      0.81      0.81      5124\n",
      "weighted avg       0.84      0.84      0.84      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bnb_cv = grid_vect(Bnb, parameters_bnb, X_train, X_test, parameters_text=parameters_vect, vect=countvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression with CountVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Fitting 6 folds for each of 240 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   48.6s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  2.8min finished\n",
      "C:\\Users\\Krishna\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.714\n",
      "Best parameters set:\n",
      "\tclf__C: 1.15\n",
      "\tclf__multi_class: 'auto'\n",
      "\tclf__penalty: 'l2'\n",
      "\tfeatures__pipe__vect__max_df: 0.5\n",
      "\tfeatures__pipe__vect__min_df: 2\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.783\n",
      "\n",
      "\n",
      "Train score with best_estimator_: 0.904\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.90      0.87      1414\n",
      "           0       0.60      0.49      0.54       437\n",
      "           1       0.72      0.67      0.69       345\n",
      "\n",
      "    accuracy                           0.78      2196\n",
      "   macro avg       0.72      0.69      0.70      2196\n",
      "weighted avg       0.77      0.78      0.78      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.92      0.97      0.94      3152\n",
      "           0       0.86      0.77      0.81      1099\n",
      "           1       0.90      0.85      0.88       873\n",
      "\n",
      "    accuracy                           0.90      5124\n",
      "   macro avg       0.89      0.86      0.88      5124\n",
      "weighted avg       0.90      0.90      0.90      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_cv = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect, vect=countvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGboost with CountVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Fitting 6 folds for each of 96 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed: 25.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.694\n",
      "Best parameters set:\n",
      "\tclf__gamma: 0.5\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 1\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 2)\n",
      "Test score with best_estimator_: 0.782\n",
      "\n",
      "\n",
      "Train score with best_estimator_: 0.872\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.92      0.87      1414\n",
      "           0       0.62      0.48      0.54       437\n",
      "           1       0.75      0.62      0.68       345\n",
      "\n",
      "    accuracy                           0.78      2196\n",
      "   macro avg       0.73      0.67      0.70      2196\n",
      "weighted avg       0.77      0.78      0.77      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.95      0.92      3152\n",
      "           0       0.80      0.71      0.75      1099\n",
      "           1       0.91      0.79      0.84       873\n",
      "\n",
      "    accuracy                           0.87      5124\n",
      "   macro avg       0.86      0.82      0.84      5124\n",
      "weighted avg       0.87      0.87      0.87      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgboost_cv = grid_vect(xgboost, parameters_xgboost, X_train, X_test, parameters_text=parameters_vect, vect=countvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Using Tf-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Fitting 6 folds for each of 120 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:   43.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.624\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.25\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 4\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 2)\n",
      "Test score with best_estimator_: 0.760\n",
      "\n",
      "\n",
      "Train score with best_estimator_: 0.830\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.97      0.86      1414\n",
      "           0       0.64      0.27      0.38       437\n",
      "           1       0.79      0.54      0.64       345\n",
      "\n",
      "    accuracy                           0.76      2196\n",
      "   macro avg       0.73      0.59      0.62      2196\n",
      "weighted avg       0.75      0.76      0.73      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.98      0.89      3152\n",
      "           0       0.85      0.52      0.65      1099\n",
      "           1       0.90      0.69      0.78       873\n",
      "\n",
      "    accuracy                           0.83      5124\n",
      "   macro avg       0.86      0.73      0.77      5124\n",
      "weighted avg       0.84      0.83      0.82      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvect = TfidfVectorizer()\n",
    "multinomialnb_tfidf = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bernoulli Naive Bayes Using Tf-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Fitting 6 folds for each of 24 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:   10.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.705\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 4\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 2)\n",
      "Test score with best_estimator_: 0.774\n",
      "\n",
      "\n",
      "Train score with best_estimator_: 0.844\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.84      0.86      1414\n",
      "           0       0.55      0.63      0.58       437\n",
      "           1       0.67      0.68      0.67       345\n",
      "\n",
      "    accuracy                           0.77      2196\n",
      "   macro avg       0.70      0.72      0.71      2196\n",
      "weighted avg       0.78      0.77      0.78      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.92      0.87      0.90      3152\n",
      "           0       0.69      0.80      0.74      1099\n",
      "           1       0.80      0.81      0.80       873\n",
      "\n",
      "    accuracy                           0.84      5124\n",
      "   macro avg       0.80      0.83      0.81      5124\n",
      "weighted avg       0.85      0.84      0.85      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bnb_tfidf = grid_vect(Bnb, parameters_bnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Using Tf-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Fitting 6 folds for each of 240 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   43.0s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  2.3min finished\n",
      "C:\\Users\\Krishna\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.683\n",
      "Best parameters set:\n",
      "\tclf__C: 1.25\n",
      "\tclf__multi_class: 'auto'\n",
      "\tclf__penalty: 'l1'\n",
      "\tfeatures__pipe__vect__max_df: 0.5\n",
      "\tfeatures__pipe__vect__min_df: 4\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.788\n",
      "\n",
      "\n",
      "Train score with best_estimator_: 0.819\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.93      0.87      1414\n",
      "           0       0.66      0.44      0.53       437\n",
      "           1       0.76      0.63      0.69       345\n",
      "\n",
      "    accuracy                           0.79      2196\n",
      "   macro avg       0.75      0.67      0.70      2196\n",
      "weighted avg       0.78      0.79      0.77      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.95      0.88      3152\n",
      "           0       0.76      0.55      0.64      1099\n",
      "           1       0.84      0.69      0.76       873\n",
      "\n",
      "    accuracy                           0.82      5124\n",
      "   macro avg       0.81      0.73      0.76      5124\n",
      "weighted avg       0.82      0.82      0.81      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logisticreg_tfidf = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xgboost Using Tf-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Fitting 6 folds for each of 96 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 30.6min\n",
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed: 39.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.685\n",
      "Best parameters set:\n",
      "\tclf__gamma: 0.5\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 3\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 2)\n",
      "Test score with best_estimator_: 0.765\n",
      "\n",
      "\n",
      "Train score with best_estimator_: 0.899\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.90      0.85      1414\n",
      "           0       0.59      0.46      0.52       437\n",
      "           1       0.71      0.62      0.66       345\n",
      "\n",
      "    accuracy                           0.77      2196\n",
      "   macro avg       0.71      0.66      0.68      2196\n",
      "weighted avg       0.75      0.77      0.76      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      0.96      0.94      3152\n",
      "           0       0.83      0.79      0.81      1099\n",
      "           1       0.93      0.82      0.87       873\n",
      "\n",
      "    accuracy                           0.90      5124\n",
      "   macro avg       0.89      0.86      0.87      5124\n",
      "weighted avg       0.90      0.90      0.90      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgboost_tfidf = grid_vect(xgboost, parameters_xgboost, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Executing Models with Hash Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression with Hash Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the vectorizers\n",
    "parameters_vect = {\n",
    "    #'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
    "    'features__pipe__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    #'features__pipe__vect__min_df': (1,2,3,4)\n",
    "}\n",
    "\n",
    "\n",
    "# Parameters for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.25, 0.5, 0.75,1,1.25)\n",
    "}\n",
    "\n",
    "\n",
    "# Parameters for LogisticRegression\n",
    "parameters_logreg = {\n",
    "    \n",
    "    'clf__C': (1.05, 1.10, 1.15,1.20,1.25),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__multi_class': ['auto']\n",
    "}\n",
    "\n",
    "# Parameter grid settings for XGBoost \n",
    "parameters_xgboost = {\n",
    "    #'clf__min_child_weight': (1,5,10),\n",
    "    'clf__gamma': (0.5,1,1.5,2),\n",
    "}\n",
    "\n",
    "# Parameter grid settings for BernoulliNB\n",
    "parameters_bnb = {\n",
    "    'clf__alpha': [1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hashvect = HashingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Fitting 6 folds for each of 20 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  5.8min finished\n",
      "C:\\Users\\Krishna\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.668\n",
      "Best parameters set:\n",
      "\tclf__C: 1.2\n",
      "\tclf__multi_class: 'auto'\n",
      "\tclf__penalty: 'l1'\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.782\n",
      "\n",
      "\n",
      "Train score with best_estimator_: 0.795\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.92      0.87      1414\n",
      "           0       0.63      0.45      0.52       437\n",
      "           1       0.75      0.62      0.68       345\n",
      "\n",
      "    accuracy                           0.78      2196\n",
      "   macro avg       0.73      0.66      0.69      2196\n",
      "weighted avg       0.77      0.78      0.77      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.94      0.87      3152\n",
      "           0       0.70      0.49      0.58      1099\n",
      "           1       0.80      0.66      0.72       873\n",
      "\n",
      "    accuracy                           0.80      5124\n",
      "   macro avg       0.77      0.70      0.73      5124\n",
      "weighted avg       0.79      0.80      0.78      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logisticreg_hv = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect, vect=Hashvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_hv = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=Hashvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did not execute as parameters mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_hv = grid_vect(xgboost, parameters_xgboost, X_train, X_test, parameters_text=parameters_vect, vect=Hashvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did not execute as parameters mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bernoulli Naive Bayes with HashVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Fitting 6 folds for each of 2 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.254\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.644\n",
      "\n",
      "\n",
      "Train score with best_estimator_: 0.615\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.64      1.00      0.78      1414\n",
      "           0       0.00      0.00      0.00       437\n",
      "           1       0.00      0.00      0.00       345\n",
      "\n",
      "    accuracy                           0.64      2196\n",
      "   macro avg       0.21      0.33      0.26      2196\n",
      "weighted avg       0.41      0.64      0.50      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      1.00      0.76      3152\n",
      "           0       0.00      0.00      0.00      1099\n",
      "           1       0.00      0.00      0.00       873\n",
      "\n",
      "    accuracy                           0.62      5124\n",
      "   macro avg       0.21      0.33      0.25      5124\n",
      "weighted avg       0.38      0.62      0.47      5124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krishna\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Bnb_hv = grid_vect(Bnb, parameters_bnb, X_train, X_test, parameters_text=parameters_vect, vect=Hashvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Test Data and Preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test = pd.read_csv(r'E:\\Semester Three UTD\\NLP\\Homework1\\test.csv',encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7322</td>\n",
       "      <td>@AmericanAir In car gng to DFW. Pulled over 1h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7323</td>\n",
       "      <td>@AmericanAir after all, the plane didnÂÃÂªt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7324</td>\n",
       "      <td>@SouthwestAir can't believe how many paying cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7325</td>\n",
       "      <td>@USAirways I can legitimately say that I would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7326</td>\n",
       "      <td>@AmericanAir still no response from AA. great ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text\n",
       "0  7322  @AmericanAir In car gng to DFW. Pulled over 1h...\n",
       "1  7323  @AmericanAir after all, the plane didnÂÃÂªt ...\n",
       "2  7324  @SouthwestAir can't believe how many paying cu...\n",
       "3  7325  @USAirways I can legitimately say that I would...\n",
       "4  7326  @AmericanAir still no response from AA. great ..."
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifying the list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "print(len(stopwords.words('english')))\n",
    "remove_from_stopwords = [\"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"needn't\", \"shan't\" \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\",'not',\"who\", \"what\", \"when\", \"why\", \"how\", \"which\", \"where\", \"whom\", \"no\", \"not\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of stopwords in English:\n",
      "{'once', 'few', 'shan', 'then', 'but', 'were', 'into', 'against', 'all', 'these', 'between', 'now', 'did', \"she's\", 'am', 'be', 'she', 'have', 'theirs', 'through', 'at', 'hers', 'they', 'as', 'doing', 'has', 'won', 'very', 'my', \"you'll\", 'by', 'over', 'its', 'an', 'your', \"that'll\", 'me', 'there', 'any', 'their', 'do', 'them', 'that', 've', \"you're\", 'i', 'you', 'him', 'own', 'll', 'will', 'such', 'again', 'm', 'are', 'some', 'if', 'up', 'while', 'he', 'the', 'most', 'themselves', 'only', 'd', 'we', 'her', 'and', 'itself', 'both', 'should', 'of', 'myself', 'a', 'or', 'those', 'aren', 'further', 'yours', 'because', 'below', 'to', 'just', \"you'd\", 'than', 'his', 'is', 'yourself', 'ma', 'about', 'other', 'on', 'y', 'himself', 't', 'above', \"you've\", 'needn', 'ours', 'been', 'ourselves', 'down', 'having', 'each', 'can', 'was', 'our', 're', 'from', 'so', 'with', 'o', \"it's\", 'more', 'in', 'it', 'same', 'out', 'under', 'after', 'until', 'for', 'yourselves', 'herself', 's', 'this', 'had', \"should've\", 'during', 'being', 'before', 'nor', 'does', 'off', 'here', 'too'}\n",
      "136\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) - set([\"who\", \"what\", \"when\", \"why\", \"how\", \"which\", \"where\", \"whom\", \"no\", \"not\", \"weren't\", \"aren't\",\"didn't\", \"wasn't\", \"couldn't\", \"hadn't\",\"hasn't\", \"doesn't\", \"shouldn't\", \"isn't\", \"wouldn't\", \"don't\", \"mightn't\", \"won't\", \"haven't\", \"mustn\", \"ain\",\"hasn\", \"weren\", \"mustn't\", \"wasn\", \"didn\", \"hadn\", \"don\", \"haven\", \"shouldn\", \"shan't\", \"isn\", \"wouldn\", \"mightn\", \"couldn\", \"needn't\", \"doesn\"])\n",
    "print(\"\\nList of stopwords in English:\")\n",
    "print (stop_words)\n",
    "print (len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stop_words]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "test['text']=test['text'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7322</td>\n",
       "      <td>americanair car gng dfw pulled ago icy roads h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7323</td>\n",
       "      <td>americanair plane didnâ âªt land identical wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7324</td>\n",
       "      <td>southwestair believe how many paying customers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7325</td>\n",
       "      <td>usairways legitimately say would rather driven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7326</td>\n",
       "      <td>americanair still response great job guys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text\n",
       "0  7322  americanair car gng dfw pulled ago icy roads h...\n",
       "1  7323  americanair plane didnâ âªt land identical wor...\n",
       "2  7324  southwestair believe how many paying customers...\n",
       "3  7325  usairways legitimately say would rather driven...\n",
       "4  7326          americanair still response great job guys"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextCounts(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def count_regex(self, pattern, tweet):\n",
    "        #finding all the substring containing the pattern in the tweet\n",
    "        return len(re.findall(pattern, tweet))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # fit method is used when specific operations need to be done on the train data, but not on the test data\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        #all the alphanumeric character\n",
    "        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n",
    "        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n",
    "        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n",
    "        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n",
    "        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?+', x))\n",
    "        count_urls = X.apply(lambda x: self.count_regex(r'https?://[^\\s]+[\\s]?', x))\n",
    "        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n",
    "        \n",
    "        data = pd.DataFrame({'count_words': count_words\n",
    "                           , 'count_mentions': count_mentions\n",
    "                           , 'count_hashtags': count_hashtags\n",
    "                           , 'count_capital_words': count_capital_words\n",
    "                           , 'count_excl_quest_marks': count_excl_quest_marks\n",
    "                           , 'count_urls': count_urls\n",
    "                           , 'count_emojis': count_emojis\n",
    "                          })\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0           13               0               0                    0   \n",
       "1           11               0               0                    0   \n",
       "2           16               0               0                    0   \n",
       "3           10               0               0                    0   \n",
       "4            6               0               0                    0   \n",
       "\n",
       "   count_excl_quest_marks  count_urls  count_emojis  \n",
       "0                       0           0             0  \n",
       "1                       0           0             0  \n",
       "2                       0           0             0  \n",
       "3                       0           0             0  \n",
       "4                       0           0             0  "
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tc = TextCounts()\n",
    "\n",
    "test_eda = tc.fit_transform(test.text)\n",
    "test_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower() \n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.stemming)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records have no words left after text cleaning\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ct = CleanText()\n",
    "sr_clean = ct.fit_transform(test.text)\n",
    "sr_clean.sample(5)\n",
    "empty_clean = sr_clean == ''\n",
    "print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\n",
    "sr_clean.loc[empty_clean] = '[no_text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records have no words left after text cleaning\n",
      "0    americanair car gng dfw pull ago ici road hold...\n",
      "1    americanair plane didnâ âªt land ident wors co...\n",
      "2    southwestair believ how mani pay custom left h...\n",
      "3    usairway legitim say would rather driven cross...\n",
      "4              americanair still respons great job guy\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "empty_clean = sr_clean == ''\n",
    "\n",
    "\n",
    "print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\n",
    "sr_clean.loc[empty_clean] = '[no_text]'\n",
    "print (sr_clean[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_words',\n",
       " 'count_mentions',\n",
       " 'count_hashtags',\n",
       " 'count_capital_words',\n",
       " 'count_excl_quest_marks',\n",
       " 'count_urls',\n",
       " 'count_emojis',\n",
       " 'clean_text']"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = test_eda\n",
    "data_test['clean_text'] = sr_clean\n",
    "data_test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaaand',\n",
       " 'aaadvantag',\n",
       " 'aaalwaysl',\n",
       " 'aacustomerservic',\n",
       " 'aadavantag',\n",
       " 'aadelay',\n",
       " 'aadv',\n",
       " 'aadvantag',\n",
       " 'aafail',\n",
       " 'aal',\n",
       " 'aaron',\n",
       " 'aarp',\n",
       " 'aateam',\n",
       " 'aback',\n",
       " 'abandon',\n",
       " 'abbrev',\n",
       " 'abc',\n",
       " 'abcletjetbluestreamfe',\n",
       " 'abcnetwork',\n",
       " 'abcnewsbayarea',\n",
       " 'abcwtvd',\n",
       " 'abduct',\n",
       " 'abi',\n",
       " 'abil',\n",
       " 'abl',\n",
       " 'aboard',\n",
       " 'abq',\n",
       " 'abroad',\n",
       " 'absolut',\n",
       " 'absorb',\n",
       " 'absurd',\n",
       " 'abt',\n",
       " 'abund',\n",
       " 'abus',\n",
       " 'abysm',\n",
       " 'acarl',\n",
       " 'acc',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accid',\n",
       " 'accident',\n",
       " 'accommod',\n",
       " 'accompani',\n",
       " 'accomplish',\n",
       " 'accord',\n",
       " 'accordingli',\n",
       " 'account',\n",
       " 'accountâ',\n",
       " 'accru',\n",
       " 'acct']"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(data_test.clean_text)\n",
    "bow = cv.transform(data_test.clean_text)\n",
    "#printing only first 20\n",
    "cv.get_feature_names()[0:50]\n",
    "#bow.sum(axis=0)\n",
    "\n",
    "#from sklearn.feature_extraction.text import HashingVectorizer\n",
    "#hv = HashingVectorizer()\n",
    "#bow = hv.fit_transform(sr_clean)\n",
    "#hv.get_feature_names()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair car gng dfw pull ago ici road hold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair plane didnâ âªt land ident wors co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>southwestair believ how mani pay custom left h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>usairway legitim say would rather driven cross...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair still respons great job guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit develop fli tmrw morn min layov earlier f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>usairway hello anyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>usairway husainhaqqani husain shld protest wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>usairway not like flightawar say plane still d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair don even give option hold say line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit announc pre board address mobil disabl re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>usairway realli embarrass when ask complimenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>southwestair not passport time trip could stil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair delay bag friend lisa pafe got bag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>southwestair didn see travel compet unus fund ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>usairway awesom door close minut flight leav m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit flew unit last month experi awesom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jetblu accept appl pay mobil enterpris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit cab ride dfw love got bag reimburs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jetblu ill call morn upset right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>southwestair aarp tfw appreci tweet back unexpect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit what statu flight bogotã houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair liter stop allow peopl wait line c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair realli want get home tonight prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>usairway hold reserv desk hour help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jetblu denver boston flight boston denver flig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair noth mother natur like poor commut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>usairway would rather plane get expos nasti co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jetblu guy gener enquir email address pleas th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit final arriv bogota good long flight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7290</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7291</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jetblu amybruni directtv oscar direct flight l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica flight leav dalla februari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7293</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>southwestair hold wait make special accommod m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7294</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair naomicoop mind what point twitter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7295</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jetblu thank which day week direct flight saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7296</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair joann san diego staff phenomen giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7297</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair thank let know appreci respons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7298</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair profit billion throw crap peopl wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7299</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit whi check onlin still wait line hour chec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7300</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jetblu want fli storm right give choic kid don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7301</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica start flight scold use overhead ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7302</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit land anchorag way fairbank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7303</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit that weak see hey virginamerica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7304</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit sit seat flight back vega chicago someon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7305</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair yesterday cancel flight flight cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7306</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair call reserv still wait someon call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7307</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit better train support staff appropri decor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7308</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit omg where bag yyzua enough shenanigan alr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7309</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair put flight get access loung ord hr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7310</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair don abil anywher not ticket counte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7311</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair shouldn happen put gear jeopardi w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7312</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>usairway miss reserv due cancel flightl flight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7313</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit supposedli deliveri believ when see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit yeah bag way per usual actual get use get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jetblu travel two kid tomorrow age domest need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jetblu info don understand whi couldn accur es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair understand whi flight day not go t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>usairway realli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unit not make connect stellar employe vicki th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7320 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0              13               0               0                    0   \n",
       "1              11               0               0                    0   \n",
       "2              16               0               0                    0   \n",
       "3              10               0               0                    0   \n",
       "4               6               0               0                    0   \n",
       "5              11               0               0                    0   \n",
       "6               3               0               0                    0   \n",
       "7              16               0               0                    0   \n",
       "8              10               0               0                    0   \n",
       "9              13               0               0                    0   \n",
       "10             12               0               0                    0   \n",
       "11             11               0               0                    0   \n",
       "12             11               0               0                    0   \n",
       "13             14               0               0                    0   \n",
       "14             14               0               0                    0   \n",
       "15             11               0               0                    0   \n",
       "16              7               0               0                    0   \n",
       "17              6               0               0                    0   \n",
       "18              8               0               0                    0   \n",
       "19              6               0               0                    0   \n",
       "20              7               0               0                    0   \n",
       "21              6               0               0                    0   \n",
       "22             13               0               0                    0   \n",
       "23             12               0               0                    0   \n",
       "24              6               0               0                    0   \n",
       "25             14               0               0                    0   \n",
       "26              7               0               0                    0   \n",
       "27             17               0               0                    0   \n",
       "28              9               0               0                    0   \n",
       "29              7               0               0                    0   \n",
       "...           ...             ...             ...                  ...   \n",
       "7290            1               0               0                    0   \n",
       "7291            9               0               0                    0   \n",
       "7292            5               0               0                    0   \n",
       "7293           13               0               0                    0   \n",
       "7294           11               0               0                    0   \n",
       "7295           12               0               0                    0   \n",
       "7296           13               0               0                    0   \n",
       "7297            6               0               0                    0   \n",
       "7298           11               0               0                    0   \n",
       "7299           12               0               0                    0   \n",
       "7300           12               0               0                    0   \n",
       "7301           11               0               0                    0   \n",
       "7302            5               0               0                    0   \n",
       "7303            6               0               0                    0   \n",
       "7304           14               0               0                    0   \n",
       "7305            8               0               0                    0   \n",
       "7306            8               0               0                    0   \n",
       "7307           14               0               0                    0   \n",
       "7308           13               0               0                    0   \n",
       "7309            9               0               0                    0   \n",
       "7310           12               0               0                    0   \n",
       "7311           12               0               0                    0   \n",
       "7312           16               0               0                    0   \n",
       "7313            6               0               0                    0   \n",
       "7314           14               0               0                    0   \n",
       "7315           12               0               0                    0   \n",
       "7316           13               0               0                    0   \n",
       "7317           12               0               0                    0   \n",
       "7318            2               0               0                    0   \n",
       "7319           15               0               0                    0   \n",
       "\n",
       "      count_excl_quest_marks  count_urls  count_emojis  \\\n",
       "0                          0           0             0   \n",
       "1                          0           0             0   \n",
       "2                          0           0             0   \n",
       "3                          0           0             0   \n",
       "4                          0           0             0   \n",
       "5                          0           0             0   \n",
       "6                          0           0             0   \n",
       "7                          0           0             0   \n",
       "8                          0           0             0   \n",
       "9                          0           0             0   \n",
       "10                         0           0             0   \n",
       "11                         0           0             0   \n",
       "12                         0           0             0   \n",
       "13                         0           0             0   \n",
       "14                         0           0             0   \n",
       "15                         0           0             0   \n",
       "16                         0           0             0   \n",
       "17                         0           0             0   \n",
       "18                         0           0             0   \n",
       "19                         0           0             0   \n",
       "20                         0           0             0   \n",
       "21                         0           0             0   \n",
       "22                         0           0             0   \n",
       "23                         0           0             0   \n",
       "24                         0           0             0   \n",
       "25                         0           0             0   \n",
       "26                         0           0             0   \n",
       "27                         0           0             0   \n",
       "28                         0           0             0   \n",
       "29                         0           0             0   \n",
       "...                      ...         ...           ...   \n",
       "7290                       0           0             0   \n",
       "7291                       0           0             0   \n",
       "7292                       0           0             0   \n",
       "7293                       0           0             0   \n",
       "7294                       0           0             0   \n",
       "7295                       0           0             0   \n",
       "7296                       0           0             0   \n",
       "7297                       0           0             0   \n",
       "7298                       0           0             0   \n",
       "7299                       0           0             0   \n",
       "7300                       0           0             0   \n",
       "7301                       0           0             0   \n",
       "7302                       0           0             0   \n",
       "7303                       0           0             0   \n",
       "7304                       0           0             0   \n",
       "7305                       0           0             0   \n",
       "7306                       0           0             0   \n",
       "7307                       0           0             0   \n",
       "7308                       0           0             0   \n",
       "7309                       0           0             0   \n",
       "7310                       0           0             0   \n",
       "7311                       0           0             0   \n",
       "7312                       0           0             0   \n",
       "7313                       0           0             0   \n",
       "7314                       0           0             0   \n",
       "7315                       0           0             0   \n",
       "7316                       0           0             0   \n",
       "7317                       0           0             0   \n",
       "7318                       0           0             0   \n",
       "7319                       0           0             0   \n",
       "\n",
       "                                             clean_text  \n",
       "0     americanair car gng dfw pull ago ici road hold...  \n",
       "1     americanair plane didnâ âªt land ident wors co...  \n",
       "2     southwestair believ how mani pay custom left h...  \n",
       "3     usairway legitim say would rather driven cross...  \n",
       "4               americanair still respons great job guy  \n",
       "5     unit develop fli tmrw morn min layov earlier f...  \n",
       "6                                  usairway hello anyon  \n",
       "7     usairway husainhaqqani husain shld protest wel...  \n",
       "8     usairway not like flightawar say plane still d...  \n",
       "9     americanair don even give option hold say line...  \n",
       "10    unit announc pre board address mobil disabl re...  \n",
       "11    usairway realli embarrass when ask complimenta...  \n",
       "12    southwestair not passport time trip could stil...  \n",
       "13    americanair delay bag friend lisa pafe got bag...  \n",
       "14    southwestair didn see travel compet unus fund ...  \n",
       "15    usairway awesom door close minut flight leav m...  \n",
       "16              unit flew unit last month experi awesom  \n",
       "17               jetblu accept appl pay mobil enterpris  \n",
       "18              unit cab ride dfw love got bag reimburs  \n",
       "19                     jetblu ill call morn upset right  \n",
       "20    southwestair aarp tfw appreci tweet back unexpect  \n",
       "21                unit what statu flight bogotã houston  \n",
       "22    americanair liter stop allow peopl wait line c...  \n",
       "23    americanair realli want get home tonight prefe...  \n",
       "24                  usairway hold reserv desk hour help  \n",
       "25    jetblu denver boston flight boston denver flig...  \n",
       "26       americanair noth mother natur like poor commut  \n",
       "27    usairway would rather plane get expos nasti co...  \n",
       "28    jetblu guy gener enquir email address pleas th...  \n",
       "29             unit final arriv bogota good long flight  \n",
       "...                                                 ...  \n",
       "7290                                               unit  \n",
       "7291  jetblu amybruni directtv oscar direct flight l...  \n",
       "7292           virginamerica flight leav dalla februari  \n",
       "7293  southwestair hold wait make special accommod m...  \n",
       "7294  americanair naomicoop mind what point twitter ...  \n",
       "7295  jetblu thank which day week direct flight saw ...  \n",
       "7296  americanair joann san diego staff phenomen giv...  \n",
       "7297         americanair thank let know appreci respons  \n",
       "7298  americanair profit billion throw crap peopl wh...  \n",
       "7299  unit whi check onlin still wait line hour chec...  \n",
       "7300  jetblu want fli storm right give choic kid don...  \n",
       "7301  virginamerica start flight scold use overhead ...  \n",
       "7302                    unit land anchorag way fairbank  \n",
       "7303               unit that weak see hey virginamerica  \n",
       "7304  unit sit seat flight back vega chicago someon ...  \n",
       "7305  americanair yesterday cancel flight flight cus...  \n",
       "7306  americanair call reserv still wait someon call...  \n",
       "7307  unit better train support staff appropri decor...  \n",
       "7308  unit omg where bag yyzua enough shenanigan alr...  \n",
       "7309  americanair put flight get access loung ord hr...  \n",
       "7310  americanair don abil anywher not ticket counte...  \n",
       "7311  americanair shouldn happen put gear jeopardi w...  \n",
       "7312  usairway miss reserv due cancel flightl flight...  \n",
       "7313           unit supposedli deliveri believ when see  \n",
       "7314  unit yeah bag way per usual actual get use get...  \n",
       "7315  jetblu travel two kid tomorrow age domest need...  \n",
       "7316  jetblu info don understand whi couldn accur es...  \n",
       "7317  americanair understand whi flight day not go t...  \n",
       "7318                                    usairway realli  \n",
       "7319  unit not make connect stellar employe vicki th...  \n",
       "\n",
       "[7320 rows x 8 columns]"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best model to be used for Prediction - logistic with Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_predict_labels = logreg_cv.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done :D\n"
     ]
    }
   ],
   "source": [
    "# Create predictions to be submitted!\n",
    "pd.DataFrame({'Id': test.id, 'Target': tweet_predict_labels}).to_csv('solution_base_logregtfidf4.csv', index =False)  \n",
    "print(\"Done :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.498224043715847"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(data.Target, tweet_predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro: 0.32479812529404684\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "f1_macro = metrics.f1_score(data.Target, tweet_predict_labels, average=\"macro\")\n",
    "print (\"f1_macro:\", f1_macro)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
